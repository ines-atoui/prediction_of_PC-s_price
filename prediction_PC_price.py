# -*- coding: utf-8 -*-
"""My project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u-2jgQN4b2xgX6N07HRGLM5ZyWtVwDmc

#1.  Import libraries
"""

import pandas as pd
import re
from sklearn import preprocessing 
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import normalize
from yellowbrick.cluster import KElbowVisualizer
import scipy.cluster.hierarchy as shc
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

#2. Loading our dataset

pc=pd.read_csv("mytek.csv")
pc

#3. Exploration and cleaning our data

##*getting a quick overview of our data*


pc.head()

pc.info()

pc.describe()

##Identifying and handling missing values

#Checking for missing values
pc.isnull().sum()

#dropping the unnecessary columns
pc=pc.drop(columns=['web-scraper-order','dispo','processeur','web-scraper-start-url','connectors','gtin','couleur'	,'select'	,'select-href',	'pagination'	,'prod',	'prod-href','gamme','systeme','taille_ecran'], axis=1)

#dropping missing values (columns with NaN)
pc=pc.dropna()

pc.isnull().sum()

pc

##Some visualization

pc['prix'].value_counts()

#dropping string of form "-?\d+DT"
prix=[]
for price in pc['prix']:
    clean_text = re.sub(r'-?\d+DT', '', price)
    prix.append(clean_text)
prix

#dropping string 'Prix spécial|Ancien prix|\xa0|DT|\u202f'
prixx=[]
for price in prix:
    clean_text = re.sub(r'Prix spécial|Ancien prix|\xa0|DT|\u202f', '', price)
    prixx.append(clean_text)
prixx

#splitting column and selecting the first element 
l1=[]
parts=[] 
for line in prixx:
    
    parts = line.split('  ')
    l1.append(parts[0])
pc['prix']=l1
pc['prix']

#replace "," by "." in the column prix
x=[]
for i in pc["prix"]:
  x.append(i.replace(",", "."))
pc['prix']=x

#convert the type of "prix" into float
pc['prix']=pc['prix'].astype("float")

#Price Distribution by Brand
sns.barplot(x=pc['marque'], y=pc['prix'])
plt.xticks(rotation='vertical')
plt.title('Distribution des prix par marque')
plt.show()

#Price Distribution by type of processeur
sns.barplot(x=pc['type_processeur'], y=pc['prix'])
plt.xticks(rotation='vertical')
plt.title('Price Distribution by type of processeur')
plt.show()

#Price Distribution by type of hard drive
sns.barplot(x=pc['type_disk'], y=pc['prix'])
plt.xticks(rotation='vertical')
plt.title('Distribution des prix par type de disque dur')
plt.show()

#Distribution of Prices Based on Warranty
sns.barplot(x=pc['grantie'], y=pc['prix'])
plt.xticks(rotation='vertical')
plt.title('Distribution des prix selon la garantie')
plt.show()

sns.barplot(x=pc['memoire_cache'], y=pc['prix'])
plt.xticks(rotation='vertical')
plt.show()

sns.barplot(x=pc['gamer'], y=pc['prix'])
plt.xticks(rotation='vertical')
plt.show()

##Formatting and transforming data

#determining potential values of each column
pc.nunique()

#converting the "grantie" and "gamer" variable into numerical data
pc['gamer']=pc['gamer'].map({"oui": 1 , "Non": 0}) 
pc['grantie']=pc['grantie'].map({"1 an": 1 , "2 ans": 2 ,"3 ans":3 }) 
pc

pc['memoire_cache'].value_counts()

pc['ram'].value_counts()

#extracting the numeric part of the two columns (memoire_cache and ram)
pc['memoire_cache']=pc['memoire_cache'].str[:-2]
pc['ram']=pc['ram'].str[:-2]

#converting the type of the data into "int"
pc['memoire_cache']=pc['memoire_cache'].astype("int")
pc['ram']=pc['ram'].astype("int")

pc

pc['disque_dur'].value_counts()

#transfering disque_dur's data from "To" to "Go"

pc1=[]
for p in pc['disque_dur']:
   pc1.append( p.replace("1 To ", "1024 Go "))
   
pc2=[]
for p in pc1:
    pc2.append( p.replace("1To ", "1024 Go "))

pc3=[]
for p in pc2:
    pc3.append( p.replace("2To ", "2048 Go "))

pc['disque_dur']=pc3

pc['carte_graphic']

#converting the variable into numerical data
  
label_encoder = LabelEncoder()

pc['disque_dur']= label_encoder.fit_transform(pc['disque_dur'])
pc['marque']= label_encoder.fit_transform(pc['marque'])
pc['type_processeur']= label_encoder.fit_transform(pc['type_processeur'])
pc['type_disk']= label_encoder.fit_transform(pc['type_disk'])
pc['carte_graphic']= label_encoder.fit_transform(pc['carte_graphic'])
pc['chipset_carte']= label_encoder.fit_transform(pc['chipset_carte'])

pc['resolution'].value_counts()

#extracting only the numeric part of the data
pc['resolution']=pc['resolution'].str[:-7]
pc['resolution']

#create two columns for the screen's "width" and "height"
pc['screen_width']=pc['resolution'].str.split("x").apply(lambda x : x[0])
pc['screen_height']=pc['resolution'].str.split("x").apply(lambda x : x[1])

#converting the type of the data into "int"
pc['screen_width']=pc['screen_width'].astype("int")
pc['screen_height']=pc['screen_height'].astype("int")

#dropping "resolution" column
pc=pc.drop(columns=['resolution'],axis=1)

pc['ref_proc'].value_counts()

#extracting string of form "i\d+"

pattern = r"i\d+"

a = []

for i in pc['ref_proc']:
    matches = re.findall(pattern, str(i))
    if matches:
        for m in matches:
            a.append(m)
    else:
        index=pc[pc['ref_proc']==i].index.tolist()[0]
        pc=pc.drop(index)

pc['ref_proc']=a

#converting the variable into numerical data
pc['ref_proc']= label_encoder.fit_transform(pc['ref_proc'])
pc['ref_proc']

# pc.to_csv('data.csv',sep=';' ,index=False, encoding="utf-8")

pc['freq_processeur'].value_counts()

#select the max of processor's frequency 
l=[]
l1=[]
for i in pc['freq_processeur']:
  substrings = i.split(' to')
  l.append(substrings)
for i in l:
  j= l.index(i)
  l1.append(l[j][1])
   
l1

#select only the numbers 
l2=[]
for i in l1:
  numbers = re.findall(r'\d\.\d+|\d\,\d+', i)
  l2.append(numbers)
print(type(l2))

#create a column containing the "max" of freq_processeur
pc['freq_processeur_max']= sum(l2, [])

#convert ',' to '.' 
x=[]
for i in pc['freq_processeur_max'] :
  x.append(i.replace(",", "."))
pc['freq_processeur_max']=x

#converting the type of the data into "float"
pc['freq_processeur_max']=pc['freq_processeur_max'].astype("float")

#droppind freq_processeur column
pc = pc.drop('freq_processeur', axis=1)

pc.info()

pc

#correlation between prix and the others features
pc.corr()['prix']

#correlation between features
plt.figure(figsize=(18,15))
sns.heatmap(pc.corr(), annot=True , cmap="YlGnBu")

#4. Modelling

##Linear Regression


# data_array = pc.values
# data_array_2d = np.reshape(data_array, (-1, data_array.shape[-1]))
# data_scaled = normalize(pc)
# data_scaled[12]

X=pc.drop(columns=["prix"])
y=pc["prix"].values[:,np.newaxis]
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=19)

print(X.shape)
print(y.shape)

model=LinearRegression()
model.fit(X_train,y_train)

#Plot the linear regression.
# plt.scatter(X,y.T,color="r")
# plt.title("Linear Regression")
# plt.ylabel("prix")
# plt.xlabel("others features")
# plt.plot(X,model.predict(X),color="b")

#Measure the performance of linear regression using the testing set. 
predicted=model.predict(X_test)
print("MSE", mean_squared_error(y_test,predicted))
print("R squared", metrics.r2_score(y_test,predicted))

##2 KNN

knn=KNeighborsClassifier(n_neighbors=15) 
knn.fit(X_train,y_train) 
y_pred=knn.predict(X_test) 
# print('Acuuracy=',accuracy_score(y_pred,y_test))
predicted=knn.predict(X_test)
print("MSE", mean_squared_error(y_test,predicted))
print("R squared", metrics.r2_score(y_test,predicted))

#The optimal number of neighbors
error_rate = []
for i in range(1,20):
 knn = KNeighborsClassifier(n_neighbors=i)
 knn.fit(X_train,y_train)
 pred_i = knn.predict(X_test)
 error_rate.append(np.mean(pred_i != y_test))
plt.plot(error_rate)

##Decision tree

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

decision_tree = DecisionTreeClassifier(criterion="gini",random_state=19)  
decision_tree.fit(X_train, y_train)  
y_pred=decision_tree.predict(X_test)  
# print("gini_accuracy:{}".format(accuracy_score(y_test, y_pred)))
predicted=decision_tree.predict(X_test)
print("MSE", mean_squared_error(y_test,predicted))
print("R squared", metrics.r2_score(y_test,predicted))

##Random F

from sklearn.ensemble import RandomForestClassifier
RF=RandomForestClassifier(n_estimators=200,random_state=19) 
RF.fit(X_train, y_train)  
y_pred=RF.predict(X_test)  
# print("Accuracy:", metrics.accuracy_score(y_test, y_pred)) 
predicted=RF.predict(X_test)
print("MSE", mean_squared_error(y_test,predicted))
print("R squared", metrics.r2_score(y_test,predicted))

data = { 'knn':0.018518518518518517, 'decision_tree':0.14814814814814814,
        'RF':0.14814814814814814}
models_name  = list(data.keys())
accuracy = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
 
plt.bar(models_name, accuracy, color ='green',
        width = 0.4)
 
plt.xlabel("models_name")
plt.ylabel("accuracy")
plt.title("a summary graph which resume all accuracies of each model")
plt.show()

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler

X=pc.drop(columns=["prix"])
y=pc["prix"]
#spliting our data
X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.1)
X_model, X_valid, y_model, y_valid = train_test_split(X_train, y_train, test_size = 0.2)

#define our model
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Training our model
model.fit(X_model, y_model, validation_data=(X_valid, y_valid), epochs= 20, callbacks=[keras.callbacks.EarlyStopping(patience=21)])

# Evaluate our model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test accuracy: {accuracy}')

#mou7awla o5ra

#splitting
X_train, X_test, y_train, y_test = train_test_split(pc.drop('prix', axis=1), pc['prix'], test_size=0.2, random_state=42)
# Scale the features
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Build the ANN model
model = Sequential()
model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=1)

# Evaluate the model
loss = model.evaluate(X_test, y_test, verbose=0)
print('Mean squared error: {:.2f}'.format(loss))

# Make predictions
y_pred = model.predict(X_test)

#5. Streamlit app

le = LabelEncoder()


le.fit(pc['marque'])
pc['memoire_cache'] = le.fit_transform(pc['memoire_cache'])



pc['marque'] = le.inverse_transform(pc['marque'])
pc['ref_proc'] = le.inverse_transform(pc['ref_proc'])
pc['memoire_cache'] = le.inverse_transform(pc['memoire_cache'])
pc['type_processeur'] = le.inverse_transform(pc['type_processeur'])
pc['carte_graphic'] = le.inverse_transform(pc['carte_graphic'])






#pip install streamlit

import streamlit as st

#title of app
st.title("prédiction du prix d'un PC")

#brand of pc
Marque=st.selectbox('Marque',pc['marque'].unique())

#pc_type(gamer or not)
Gamer=st.selectbox('Gamer',['Oui','Non'])

#ref_proc
ref_proc=st.selectbox('référence_du_processeur',pc['ref_proc'].unique())

#memoire_cache
mm_cache=st.selectbox('Memoire_cache(en Mo)',pc['memoire_cache'])

#frq_proc
frq_proc=st.selectbox('Fréquence_du_processeur_maximale',pc['freq_processeur_max'])

#carte_graph
carte_graph=st.selectbox('carte_graphique',pc['carte_graphic'].unique())


if st.button('prédiction du prix') :
  pass

